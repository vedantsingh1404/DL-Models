{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMxyLv_F0xp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UinC-uQ1DS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available :\n",
        "  device = torch.device('cuda:0')\n",
        "else :\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "# print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q40a1xrD1jVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "586744b5-77f6-42a1-bb98-37611a82d75c"
      },
      "source": [
        "train_data = torchvision.datasets.MNIST(root = './data', train = True, download = True, transform = transforms.ToTensor())\n",
        "test_data = torchvision.datasets.MNIST(root = './data', train = False, download = True, transform = transforms.ToTensor())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3554476.14it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 56189.55it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 943846.16it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21162.65it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7xWfYXotgXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = 100, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = 100, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wclQZBvVu8Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(20, 40, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(7 * 7 * 40, 512),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(512, 128),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(128, 10))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf_lD5AfxsXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "alpha = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fqhs9-DxJhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn = ConvNN().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kohNhgE_xTFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(cnn.parameters(), lr = alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSwIcUtFEPY3",
        "colab_type": "code",
        "outputId": "a9a23448-08c5-4c79-ab48-fc326cc900df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "total_step = len(train_loader)\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = cnn.forward(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        \n",
        "    print('Epoch : {} / {}  |  Loss : {:.6f}'.format(epoch + 1, epochs, loss))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 / 10  |  Loss : 0.369924\n",
            "Epoch : 2 / 10  |  Loss : 0.137392\n",
            "Epoch : 3 / 10  |  Loss : 0.068457\n",
            "Epoch : 4 / 10  |  Loss : 0.049072\n",
            "Epoch : 5 / 10  |  Loss : 0.060253\n",
            "Epoch : 6 / 10  |  Loss : 0.020193\n",
            "Epoch : 7 / 10  |  Loss : 0.050488\n",
            "Epoch : 8 / 10  |  Loss : 0.013504\n",
            "Epoch : 9 / 10  |  Loss : 0.023681\n",
            "Epoch : 10 / 10  |  Loss : 0.029154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwVm6QXI-uWR",
        "colab_type": "code",
        "outputId": "749bec36-33bf-4310-f1cf-b6e6d628892a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = cnn(images)\n",
        "        pred = torch.argmax(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "    print('correct : {}'.format(correct))\n",
        "    print('total : {}'.format(total))\n",
        "    print('accuracy : {}'.format(100 * correct / total))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct : 9914\n",
            "total : 10000\n",
            "accuracy : 99.14\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}